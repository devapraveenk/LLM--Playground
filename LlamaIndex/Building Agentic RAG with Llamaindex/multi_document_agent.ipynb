{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Document Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv #load the API-KEY\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://openreview.net/pdf?id=VtmBAGCN7o\",\n",
    "    \"https://openreview.net/pdf?id=6PmJoRfdaK\",\n",
    "    \"https://openreview.net/pdf?id=LzPWWPAdY4\",\n",
    "    \"https://openreview.net/pdf?id=VTF8yNQM66\",\n",
    "    \"https://openreview.net/pdf?id=hSyW5go0v8\",\n",
    "    \"https://openreview.net/pdf?id=9WD9KwssyT\",\n",
    "    \"https://openreview.net/pdf?id=yV6fD7LYkF\",\n",
    "    \"https://openreview.net/pdf?id=hnrB5YHoYu\",\n",
    "    \"https://openreview.net/pdf?id=WbWtOYIzIK\",\n",
    "    \"https://openreview.net/pdf?id=c5pwL0Soay\",\n",
    "    \"https://openreview.net/pdf?id=TpD2aG1h0D\"\n",
    "]\n",
    "\n",
    "#download the papers using URL\n",
    "\n",
    "\n",
    "papers = [\n",
    "    \"metagpt.pdf\",\n",
    "    \"longlora.pdf\",\n",
    "    \"loftq.pdf\",\n",
    "    \"swebench.pdf\",\n",
    "    \"selfrag.pdf\",\n",
    "    \"zipformer.pdf\",\n",
    "    \"values.pdf\",\n",
    "    \"finetune_fair_diffusion.pdf\",\n",
    "    \"knowledge_card.pdf\",\n",
    "    \"metra.pdf\",\n",
    "    \"vr_mcl.pdf\"\n",
    "]\n",
    "#for Jupyter notebook\n",
    "# for url, paper in zip(urls, papers):\n",
    "#     !wget \"{url}\" -O \"{paper}\"\n",
    "\n",
    "#General\n",
    "import os\n",
    "import requests\n",
    "\n",
    "\n",
    "target_directory = \"../src/multi_document\"\n",
    "\n",
    "os.makedirs(target_directory, exist_ok=True)\n",
    "\n",
    "# Download papers\n",
    "for url, paper in zip(urls, papers):\n",
    "    \n",
    "    paper_path = os.path.join(target_directory, os.path.basename(paper))\n",
    "    \n",
    "    print(f\"Downloading {url} to {paper_path}...\")\n",
    "    \n",
    "    # Download the file\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        with open(paper_path, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "        print(f\"Saved: {paper_path}\")\n",
    "    else:\n",
    "        print(f\"Failed to download {url} (Status code: {response.status_code})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, SummaryIndex\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.tools import FunctionTool, QueryEngineTool\n",
    "from llama_index.core.vector_stores import MetadataFilters, FilterCondition\n",
    "from typing import List, Optional\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "Settings.llm = OpenAI(model='gpt-4o-mini')\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-ada-002\") \n",
    "\n",
    "def get_doc_tools(\n",
    "    file_path: str,\n",
    "    name: str,\n",
    ") -> str:\n",
    "    \"\"\"Get vector query and summary query tools from a document.\"\"\"\n",
    "\n",
    "    # load documents\n",
    "    documents = SimpleDirectoryReader(input_files=[file_path]).load_data()\n",
    "    splitter = SentenceSplitter(chunk_size=1024)\n",
    "    nodes = splitter.get_nodes_from_documents(documents)\n",
    "    vector_index = VectorStoreIndex(nodes)\n",
    "    \n",
    "    def vector_query(\n",
    "        query: str, \n",
    "        page_numbers: Optional[List[str]] = None\n",
    "    ) -> str:\n",
    "        \"\"\"Use to answer questions over a given paper.\n",
    "    \n",
    "        Useful if you have specific questions over the paper.\n",
    "        Always leave page_numbers as None UNLESS there is a specific page you want to search for.\n",
    "    \n",
    "        Args:\n",
    "            query (str): the string query to be embedded.\n",
    "            page_numbers (Optional[List[str]]): Filter by set of pages. Leave as NONE \n",
    "                if we want to perform a vector search\n",
    "                over all pages. Otherwise, filter by the set of specified pages.\n",
    "        \n",
    "        \"\"\"\n",
    "    \n",
    "        page_numbers = page_numbers or []\n",
    "        metadata_dicts = [\n",
    "            {\"key\": \"page_label\", \"value\": p} for p in page_numbers\n",
    "        ]\n",
    "        \n",
    "        query_engine = vector_index.as_query_engine(\n",
    "            similarity_top_k=2,\n",
    "            filters=MetadataFilters.from_dicts(\n",
    "                metadata_dicts,\n",
    "                condition=FilterCondition.OR\n",
    "            )\n",
    "        )\n",
    "        response = query_engine.query(query)\n",
    "        return response\n",
    "        \n",
    "    \n",
    "    vector_query_tool = FunctionTool.from_defaults(\n",
    "        name=f\"vector_tool_{name}\",\n",
    "        fn=vector_query\n",
    "    )\n",
    "    \n",
    "    summary_index = SummaryIndex(nodes)\n",
    "    summary_query_engine = summary_index.as_query_engine(\n",
    "        response_mode=\"tree_summarize\",\n",
    "        use_async=True,\n",
    "    )\n",
    "    summary_tool = QueryEngineTool.from_defaults(\n",
    "        name=f\"summary_tool_{name}\",\n",
    "        query_engine=summary_query_engine,\n",
    "        description=(\n",
    "            f\"Useful for summarization questions related to {name}\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    return vector_query_tool, summary_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tools for paper: metagpt.pdf\n",
      "Getting tools for paper: longlora.pdf\n",
      "Getting tools for paper: loftq.pdf\n",
      "Getting tools for paper: swebench.pdf\n",
      "Getting tools for paper: selfrag.pdf\n",
      "Getting tools for paper: zipformer.pdf\n",
      "Getting tools for paper: values.pdf\n",
      "Getting tools for paper: finetune_fair_diffusion.pdf\n",
      "Getting tools for paper: knowledge_card.pdf\n",
      "Getting tools for paper: metra.pdf\n",
      "Getting tools for paper: vr_mcl.pdf\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "paper_to_tools_dict = {}\n",
    "for paper in papers:\n",
    "    print(f\"Getting tools for paper: {paper}\")\n",
    "    vector_tool, summary_tool = get_doc_tools(f'../src/multi_document/{paper}', Path(paper).stem) #arg: filepath and name (filepath, filename)\n",
    "    paper_to_tools_dict[paper] = [vector_tool, summary_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tools = [t for paper in papers for t in paper_to_tools_dict[paper]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an \"object\" index and retriever over these tools\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.objects import ObjectIndex\n",
    "\n",
    "obj_index = ObjectIndex.from_objects(\n",
    "    all_tools,\n",
    "    index_cls=VectorStoreIndex,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_retriever = obj_index.as_retriever(similarity_top_k=3) #retrive the top 3 similar agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = obj_retriever.retrieve(\n",
    "    \"Tell me about the eval dataset used in MetaGPT and SWE-Bench\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ToolMetadata(description='Useful for summarization questions related to metagpt', name='summary_tool_metagpt', fn_schema=<class 'llama_index.core.tools.types.DefaultToolFnSchema'>, return_direct=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools[0].metadata # look into the retrival process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model='gpt-4o-mini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import AgentRunner\n",
    "\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    tool_retriever=obj_retriever,\n",
    "    llm=llm, \n",
    "    system_prompt=\"\"\" \n",
    "You are an agent designed to answer queries over a set of given papers.\n",
    "Please always use the tools provided to answer a question. Do not rely on prior knowledge.\\\n",
    "\n",
    "\"\"\",\n",
    "    verbose=True\n",
    ")\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me about the evaluation dataset used in MetaGPT and compare it against SWE-Bench\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_metagpt with args: {\"input\": \"evaluation dataset used in MetaGPT\"}\n",
      "=== Function Output ===\n",
      "The evaluation of MetaGPT utilizes two public benchmarks: HumanEval and MBPP. HumanEval consists of 164 handwritten programming tasks that include function specifications, descriptions, reference codes, and tests. MBPP comprises 427 Python tasks covering core concepts and standard library features, also including descriptions, reference codes, and automated tests. Additionally, a self-generated dataset named SoftwareDev is used, which includes 70 diverse software development tasks, each with its own task prompt.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_swebench with args: {\"input\": \"evaluation dataset used in SWE-Bench\"}\n",
      "=== Function Output ===\n",
      "The evaluation dataset used in SWE-bench consists of 2,294 software engineering problems derived from real GitHub issues and corresponding pull requests across 12 popular Python repositories. Each task involves providing a codebase and a description of an issue that needs to be resolved. The models are tasked with generating a patch to address the issue, which is then evaluated based on whether it passes the associated tests. The dataset is designed to reflect realistic software engineering challenges, requiring models to understand and coordinate changes across multiple functions, classes, and files.\n",
      "=== LLM Response ===\n",
      "### Evaluation Dataset in MetaGPT\n",
      "\n",
      "MetaGPT utilizes two public benchmarks for its evaluation:\n",
      "\n",
      "1. **HumanEval**: This benchmark consists of 164 handwritten programming tasks that include function specifications, descriptions, reference codes, and tests.\n",
      "2. **MBPP (Multi-Branch Programming Problems)**: This dataset comprises 427 Python tasks that cover core concepts and standard library features, also including descriptions, reference codes, and automated tests.\n",
      "3. **SoftwareDev**: A self-generated dataset that includes 70 diverse software development tasks, each with its own task prompt.\n",
      "\n",
      "### Evaluation Dataset in SWE-Bench\n",
      "\n",
      "SWE-Bench's evaluation dataset consists of 2,294 software engineering problems derived from real GitHub issues and corresponding pull requests across 12 popular Python repositories. Each task involves providing a codebase and a description of an issue that needs to be resolved. The models are tasked with generating a patch to address the issue, which is then evaluated based on whether it passes the associated tests. This dataset is designed to reflect realistic software engineering challenges, requiring models to understand and coordinate changes across multiple functions, classes, and files.\n",
      "\n",
      "### Comparison\n",
      "\n",
      "- **Source of Tasks**: \n",
      "  - MetaGPT uses a combination of public benchmarks (HumanEval and MBPP) and a self-generated dataset (SoftwareDev), while SWE-Bench is based on real-world GitHub issues and pull requests.\n",
      "  \n",
      "- **Number of Tasks**: \n",
      "  - MetaGPT has a total of 661 tasks (164 from HumanEval, 427 from MBPP, and 70 from SoftwareDev), whereas SWE-Bench has a larger dataset with 2,294 tasks.\n",
      "\n",
      "- **Nature of Tasks**: \n",
      "  - The tasks in MetaGPT focus on programming tasks with specific function specifications and automated tests, while SWE-Bench emphasizes realistic software engineering problems that require understanding of existing codebases and generating patches.\n",
      "\n",
      "- **Evaluation Criteria**: \n",
      "  - MetaGPT evaluates based on the correctness of the programming tasks, while SWE-Bench evaluates the generated patches based on whether they pass associated tests, reflecting a more integrated approach to software engineering challenges.\n",
      "\n",
      "In summary, while both datasets aim to evaluate programming capabilities, they differ significantly in their sources, task nature, and evaluation methodologies.\n",
      "### Evaluation Dataset in MetaGPT\n",
      "\n",
      "MetaGPT utilizes two public benchmarks for its evaluation:\n",
      "\n",
      "1. **HumanEval**: This benchmark consists of 164 handwritten programming tasks that include function specifications, descriptions, reference codes, and tests.\n",
      "2. **MBPP (Multi-Branch Programming Problems)**: This dataset comprises 427 Python tasks that cover core concepts and standard library features, also including descriptions, reference codes, and automated tests.\n",
      "3. **SoftwareDev**: A self-generated dataset that includes 70 diverse software development tasks, each with its own task prompt.\n",
      "\n",
      "### Evaluation Dataset in SWE-Bench\n",
      "\n",
      "SWE-Bench's evaluation dataset consists of 2,294 software engineering problems derived from real GitHub issues and corresponding pull requests across 12 popular Python repositories. Each task involves providing a codebase and a description of an issue that needs to be resolved. The models are tasked with generating a patch to address the issue, which is then evaluated based on whether it passes the associated tests. This dataset is designed to reflect realistic software engineering challenges, requiring models to understand and coordinate changes across multiple functions, classes, and files.\n",
      "\n",
      "### Comparison\n",
      "\n",
      "- **Source of Tasks**: \n",
      "  - MetaGPT uses a combination of public benchmarks (HumanEval and MBPP) and a self-generated dataset (SoftwareDev), while SWE-Bench is based on real-world GitHub issues and pull requests.\n",
      "  \n",
      "- **Number of Tasks**: \n",
      "  - MetaGPT has a total of 661 tasks (164 from HumanEval, 427 from MBPP, and 70 from SoftwareDev), whereas SWE-Bench has a larger dataset with 2,294 tasks.\n",
      "\n",
      "- **Nature of Tasks**: \n",
      "  - The tasks in MetaGPT focus on programming tasks with specific function specifications and automated tests, while SWE-Bench emphasizes realistic software engineering problems that require understanding of existing codebases and generating patches.\n",
      "\n",
      "- **Evaluation Criteria**: \n",
      "  - MetaGPT evaluates based on the correctness of the programming tasks, while SWE-Bench evaluates the generated patches based on whether they pass associated tests, reflecting a more integrated approach to software engineering challenges.\n",
      "\n",
      "In summary, while both datasets aim to evaluate programming capabilities, they differ significantly in their sources, task nature, and evaluation methodologies.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"Tell me about the evaluation dataset used \"\n",
    "    \"in MetaGPT and compare it against SWE-Bench\"\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Compare and contrast the LoRA papers (LongLoRA, LoftQ). Analyze the approach in each paper first. \n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_longlora with args: {\"input\": \"LongLoRA is a method that focuses on efficient fine-tuning of large language models (LLMs) using low-rank adaptation (LoRA). The approach involves decomposing the weight updates into low-rank matrices, which significantly reduces the number of trainable parameters. This allows for faster training and lower memory usage while maintaining performance. LongLoRA specifically addresses the challenges of long-context modeling by incorporating mechanisms that enhance the model's ability to handle longer sequences without a proportional increase in computational cost.\"}\n",
      "=== Function Output ===\n",
      "LongLoRA is an efficient fine-tuning method designed for large language models (LLMs) that utilizes low-rank adaptation (LoRA) to optimize the training process. By decomposing weight updates into low-rank matrices, it significantly reduces the number of trainable parameters, leading to faster training times and lower memory requirements while preserving model performance. This approach specifically targets the difficulties associated with long-context modeling, implementing mechanisms that improve the model's capacity to manage longer sequences without a corresponding rise in computational costs.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_loftq with args: {\"input\": \"LoftQ is a technique that also aims to improve the efficiency of fine-tuning large language models but does so by introducing quantization methods. The approach focuses on reducing the precision of the model weights and activations, which leads to a decrease in memory usage and computational requirements. LoftQ emphasizes maintaining model performance while enabling the deployment of LLMs on resource-constrained devices. The method includes strategies for quantizing the model in a way that minimizes the loss of accuracy.\"}\n",
      "=== Function Output ===\n",
      "LoftQ is a quantization framework designed to enhance the efficiency of fine-tuning large language models (LLMs) by integrating quantization techniques with LoRA fine-tuning. This approach reduces the precision of model weights and activations, resulting in lower memory usage and computational demands. LoftQ aims to maintain model performance while facilitating the deployment of LLMs on devices with limited resources. It employs strategies that ensure quantization is performed in a manner that minimizes accuracy loss, thereby improving generalization in various downstream tasks.\n",
      "=== LLM Response ===\n",
      "### Comparison of Approaches in LongLoRA and LoftQ\n",
      "\n",
      "#### LongLoRA\n",
      "- **Methodology**: LongLoRA employs low-rank adaptation (LoRA) for efficient fine-tuning of large language models (LLMs). It decomposes weight updates into low-rank matrices, which significantly reduces the number of trainable parameters.\n",
      "- **Benefits**: This approach leads to faster training times and lower memory usage while maintaining model performance. It specifically addresses challenges related to long-context modeling, enhancing the model's ability to handle longer sequences without a proportional increase in computational costs.\n",
      "\n",
      "#### LoftQ\n",
      "- **Methodology**: LoftQ integrates quantization techniques with LoRA fine-tuning to improve the efficiency of LLMs. It focuses on reducing the precision of model weights and activations, which decreases memory usage and computational requirements.\n",
      "- **Benefits**: LoftQ aims to maintain model performance while enabling deployment on resource-constrained devices. It includes strategies to ensure that quantization minimizes accuracy loss, thereby improving generalization across various downstream tasks.\n",
      "\n",
      "### Contrast\n",
      "- **Focus**: LongLoRA primarily targets the efficiency of training with a focus on long-context modeling, while LoftQ emphasizes quantization to reduce resource demands for deployment.\n",
      "- **Techniques Used**: LongLoRA uses low-rank adaptation to optimize parameter updates, whereas LoftQ employs quantization to lower the precision of weights and activations.\n",
      "- **Application**: LongLoRA is more suited for scenarios requiring efficient training of models that handle long sequences, while LoftQ is tailored for deploying models in environments with limited computational resources.\n",
      "\n",
      "In summary, both papers aim to enhance the efficiency of fine-tuning large language models but do so through different methodologies and with different focal points.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"Compare and contrast the LoRA papers (LongLoRA, LoftQ). \"\n",
    "    \"Analyze the approach in each paper first. \"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "generativeai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
